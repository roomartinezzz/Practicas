# -*- coding: utf-8 -*-
"""EDA Scoring banco.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W6p0LCRSzLqxi0CL-Z6hJwiZoKI44Um1

#Análisis exploratorio de datos bancarios: se intenta descubrir que casos serán futuros deudores.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

from google.colab import files

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression

warnings.warn('ignore')

datos=pd.read_csv('/content/credito.csv')
datos.head()

datos.columns

datos.info()

datos.describe().T

#PARA VER QUE NULOS HAY
print("columnas que poseen nulos o Nan")
display(datos.isnull().any())

"""Eiminamos cant consultas 7 dias porque no sabemos a que se refiere y prov codigo ya que esta repetida."""

datos.drop(['provincia_codigo'],axis=1,inplace=True)

datos.set_index(['ID'],inplace=True)
datos

datos['autonomo'].unique()

datos['Es_jubilado'].unique()

"""Hay inconsistencias, que significa 0? Se supone que es Falso."""

datos['autonomo']=datos.autonomo.map({'F' : 0,'V' : 1})
datos['Es_jubilado']=datos.Es_jubilado.map({'F': 0,'V' : 1})

print(datos['autonomo'].dtype)

datos['Es_jubilado'].dtype

print(datos['autonomo'].unique())
print(datos['Es_jubilado'].unique())

datos.info()

"""Se supone que los valores que faltan en autonomos y jubilados son valores que corresponden a 0. Se procede a reemplazarlos"""

datos.fillna(0, inplace=True)
datos.info()

datos.autonomo.value_counts()

datos.Es_jubilado.value_counts()

datos.relacion_dependencia.value_counts()

"""No es conveniente tener donde trabaja cada persona, no suma al analisis. Entonces en cada caso donde exista un lugar de trabajo se supone que la persona esta en relación de dependencia, en caso contrario no.

Para modificarlo, se utiliza una máscara
"""

#  existen 3 columnas del tipo object
categoria_mask=datos.dtypes == object
#  aca las pinchamos
categorica_columns= datos.columns[categoria_mask].tolist()

# defino las numericas como el complemento de las categoricas
numerical_columns=datos.columns[~datos.columns.isin(categorica_columns)]
print("tenemos %s variables numéricas " % len(numerical_columns))
print("tenemos %s variables categoricas " % len(categorica_columns))
categorica_columns

print('Numero de categorias por variable: ')
datos[categorica_columns].apply(lambda x : x.nunique())

# Para provincia y producto aplicamos un encoder  -- OneHotEncoder
# Para relacion de dependencia describe cada lugar de trabajo, asignamos 1 al que tenga un lugar y 0 a quien no.
# Por otro lado BCA_Peor_sit si hay categorias aunque la variable no sea  entera, lo mismo para Nivel_Socieconomico --- aplico OneHot
datos['relacion_dependencia'].mask(datos['relacion_dependencia']!=0, 1, inplace=True)
print('Rel_Dep\n', datos.relacion_dependencia.value_counts())
print('BCRA\n', datos.BCRA_Peor_Situacion.value_counts())
print('NivSE\n', datos.Nivel_Socioeconomico.value_counts())
datos.info()

# Codificado
# PRIMERO HAY CAMBIAR EL TIPO DE LAS COLUMNAS A TRANSFORMAR A 'object' O 'category' O 'STRING'

columnas_cod=['PRODUCTO', 'PROVINCIA', 'BCRA_Peor_Situacion', 'Nivel_Socioeconomico']
datos[columnas_cod] = datos[columnas_cod].astype(str)
datos[['relacion_dependencia', 'autonomo', 'Es_jubilado']]=datos[['relacion_dependencia', 'autonomo', 'Es_jubilado']].astype(int)  #  son enteros pero sin tipo int
datos.info()

#AHORA SI APLICAMOS EL CODIFICADO
datos=pd.get_dummies(datos, dtype=int)
datos.info()

# Veamos si hay correlaciones
colormap = plt.cm.viridis   # mapa de colores
plt.figure(figsize = (25,25))
plt.title('Correlación de Pearson de las características', y = 1.05, size = 15)
sns.heatmap(datos.corr(), linewidths = 0.1, vmax = 1.0, square = True,
            cmap = colormap,linecolor = 'white', annot = True)
plt.show()

#Distribución de  la categoría target 'moroso'
print(datos.groupby('moroso').size())
sns.catplot(x='moroso', data= datos,kind='count')

"""Ahora el dataset ya se encuentra limpio, y estamos en condiciones de realizar modelos de machine learning."""

# 1)
credito=datos.copy()
y=credito['moroso']
X=credito.drop('moroso',axis=1)

# 2) Separacion en train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y,
                                                    random_state = 6)

# estandarizo los datos
pipe_log=make_pipeline(StandardScaler(),
                       LogisticRegression(random_state=16, l1_ratio=0.5, max_iter=500))


param_range = [0.01, 0.1, 1.0, 10.0, 100.0]

param_grid = [{'logisticregression__C': param_range, 'logisticregression__penalty':['l1', 'l2', 'elasticnet'],
               'logisticregression__solver':['lbfgs', 'liblinear', 'newton-cg']
               }]

gs = GridSearchCV(estimator=pipe_log,
                  param_grid=param_grid,
                  scoring='accuracy',
                  cv=10,
                  n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(f'Mejor accuracy: {gs.best_score_*100} %')
print(f'Mejores parámetros: {gs.best_params_}')

clf = gs.best_estimator_
clf.fit(X_train, y_train)
print('Test accuracy: %.3f' % clf.score(X_test, y_test))

#matriz de confusión
y_pred_log=clf.predict(X_test)
print(f'tn, fp, fn, tp = {confusion_matrix(y_test, y_pred_log).ravel()}')

conf1 = confusion_matrix(y_test, y_pred_log, normalize='all')
ConfusionMatrixDisplay(conf1).plot()